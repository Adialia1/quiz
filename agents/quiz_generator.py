"""
Quiz Generator Agent
Generates personalized exam questions based on user preferences

Uses:
- Exam RAG: Reference existing questions for style/format
- Legal RAG: Ensure legal accuracy and proper citations
- Thinking Model: Maximum capability for question generation
"""
from typing import Dict, Any, Optional, List
import json
import re

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from agents.base_agent import RAGAgent
from rag.exam_rag import ExamRAG
from rag.legal_rag import LegalRAG
from config.settings import THINKING_MODEL
from openai import OpenAI
from config.settings import OPENROUTER_API_KEY

# Import Legal Expert for validation
from agents.legal_expert import LegalExpertAgent


class QuizGeneratorAgent(RAGAgent):
    """
    Quiz Generator Agent - Creates personalized exam questions

    Capabilities:
    - Generate questions covering all topics or specific weak points
    - Use exam database for reference questions
    - Use legal database for accurate citations
    - Create questions with full explanations
    - Adjust difficulty level
    - Ensure proper Hebrew legal terminology
    """

    def __init__(self):
        """
        Initialize Quiz Generator Agent with maximum capabilities
        """
        # Initialize Exam RAG
        self.exam_rag = ExamRAG()

        # Initialize Legal RAG for accuracy
        legal_rag = LegalRAG()

        # Use thinking model with high parameters for best quality
        super().__init__(
            agent_name="Quiz Generator",
            system_prompt=self._build_system_prompt(),
            rag_system=legal_rag,  # Primary RAG for legal accuracy
            model=THINKING_MODEL,
            temperature=0.7,  # Higher for creativity in question generation
            top_k=20  # Maximum chunks for comprehensive context
        )

        # Direct OpenAI client for advanced calls
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=OPENROUTER_API_KEY
        )

        # Initialize Legal Expert for validation
        self.legal_expert = LegalExpertAgent()

    def _build_system_prompt(self) -> str:
        """Build comprehensive system prompt for quiz generation"""
        return """××ª×” ××•××—×” ×‘× ×™×™×¨×•×ª ×¢×¨×š ×•××ª×™×§×” ××§×¦×•×¢×™×ª, ×•××ª××—×” ×‘×™×¦×™×¨×ª ×©××œ×•×ª ××‘×—×Ÿ ××™×›×•×ª×™×•×ª.

**×ª×¤×§×™×“×š:**
×œ×™×¦×•×¨ ×©××œ×•×ª ××‘×—×Ÿ ×‘×¨××” ×’×‘×•×”×” ×‘× ×•×©× × ×™×™×¨×•×ª ×¢×¨×š, ××ª×™×§×”, ×•×—×•×§×™ ×©×•×§ ×”×”×•×Ÿ ×‘×™×©×¨××œ.

**×¡×’× ×•×Ÿ ×©××œ×•×ª - ×—×©×•×‘ ×××•×“:**
×™×© ×œ×™×™×¦×¨ **×©× ×™ ×¡×•×’×™ ×©××œ×•×ª**:

**1. ×©××œ×•×ª ×¡×™×¤×•×¨ (70-80%)** - ×©××œ×•×ª ×¢× ×ª×¨×—×™×© ×•×“××•×™×•×ª:
- "×©×•×§×™ ×•××•×§×™ ×”× ×©×•×ª×¤×™× ×‘×—×‘×¨×ª ×™×™×¢×•×¥ ×”×©×§×¢×•×ª. ×™×•× ××—×“, ×©×•×§×™ ×©××¢ ××™×“×¢ ×¢×œ ×¨×›×™×©×” ×¦×¤×•×™×”..."
- "×“×™×œ×Ÿ ×¢×•×‘×“ ×›×™×•×¢×¥ ×”×©×§×¢×•×ª ×‘×—×‘×¨×” ×’×“×•×œ×”. ×œ×§×•×— ×¤× ×” ××œ×™×• ×•×‘×™×§×©..."
- "×‘×¨× ×“×” ×× ×”×œ×ª ×ª×™×§ ×”×©×§×¢×•×ª ×œ×œ×§×•×—×•×ª ×¤×¨×˜×™×™×. ××—×“ ××œ×§×•×—×•×ª×™×” ×‘×™×§×© ××× ×”..."

**2. ×©××œ×•×ª ×‘×¡×™×¡×™×•×ª (20-30%)** - ×©××œ×•×ª ×”×‘× ×” ×‘×¡×™×¡×™×ª:
- "××”×™ ×”×’×“×¨×ª '××™×“×¢ ×¤× ×™×' ×œ×¤×™ ×—×•×§ × ×™×™×¨×•×ª ×¢×¨×š?"
- "××”×Ÿ ×—×•×‘×•×ª ×”×’×™×œ×•×™ ×©×œ ×—×‘×¨×” ×¦×™×‘×•×¨×™×ª?"
- "××™×–×” ×¡×•×’ ××™×“×¢ × ×—×©×‘ ×œ××™×“×¢ ××”×•×ª×™?"

**×¢×§×¨×•× ×•×ª ×©××œ×•×ª ×¡×™×¤×•×¨:**
1. **×“××•×™×•×ª ×‘×¢×œ×•×ª ×©××•×ª** - ×”×©×ª××© ×‘×©××•×ª ×›××• ×©×•×§×™, ××•×§×™, ×“×™×œ×Ÿ, ×‘×¨× ×“×”, ×¨×•× ×™, ×’×™× ×•×›×•'
2. **×ª×¨×—×™×© ××¦×™××•×¡×˜×™** - ×¦×•×¨ ×¡×™×˜×•××¦×™×” ×”×’×™×•× ×™×ª ××¢×•×œ× ×”×”×©×§×¢×•×ª ×•×”×¤×™× × ×¡×™×
3. **×”×§×©×¨ ×¢×©×™×¨** - ×ª×Ÿ ×¨×§×¢ ×œ××¦×‘ ×œ×¤× ×™ ×”×©××œ×” ×¢×¦××”
4. **××§×¨×” ×¡×¤×¦×™×¤×™** - ×œ× "××”×• X?" ××œ× "×‘××¦×‘ ×–×”, ××” × ×›×•×Ÿ?"
5. **×™×™×©×•× ×—×•×§** - ×‘×“×•×§ ×”×‘× ×” ×©×œ ×™×™×©×•× ×”×—×•×§, ×œ× ×¨×§ ×™×“×™×¢×ª×•

**×—×©×•×‘ ×××•×“ - ×©××•×ª ×—×‘×¨×•×ª:**
- **××¡×•×¨** ×œ×”×©×ª××© ×‘×©××•×ª ×—×‘×¨×•×ª ×××™×ª×™×•×ª (×’×•×’×œ, ××¤×œ, ××™×§×¨×•×¡×•×¤×˜, ×××–×•×Ÿ, ×˜×¡×œ×”, ×•×›×•')
- **×—×•×‘×”** ×œ×”×©×ª××© ×‘×©××•×ª ×—×‘×¨×•×ª **×‘×“×•×™×™×** ×‘×œ×‘×“
- ×“×•×’×××•×ª ×œ×©××•×ª ×‘×“×•×™×™×: "×—×‘×¨×ª ×˜×§-×§×•× ×‘×¢\"×", "××•×¤×˜×™-×¡×™×™×‘×¨ ×˜×›× ×•×œ×•×’×™×•×ª", "×¤×™× × ×¡-×™×©×¨××œ ×”×©×§×¢×•×ª", "×“×™×’×™×˜×œ-×•×™×–'×Ÿ ×‘×¢\"×"
- ×–×›×•×¨: ×–×• ×”×’× ×” ××¤× ×™ ×‘×¢×™×•×ª ×–×›×•×™×•×ª ×™×•×¦×¨×™× (copyright)

**×¢×§×¨×•× ×•×ª ×™×¦×™×¨×ª ×©××œ×•×ª:**
1. **×“×™×•×§ ××©×¤×˜×™** - ×›×œ ×©××œ×” ×—×™×™×‘×ª ×œ×”×™×•×ª ××“×•×™×§×ª ××‘×—×™× ×” ××©×¤×˜×™×ª ×•××‘×•×¡×¡×ª ×¢×œ ×”×—×•×§ ×”×™×©×¨××œ×™
2. **×‘×”×™×¨×•×ª** - ×”×©××œ×” ×—×™×™×‘×ª ×œ×”×™×•×ª ×‘×¨×•×¨×” ×•×—×“-××©××¢×™×ª
3. **××¤×©×¨×•×™×•×ª ××•×’×“×¨×•×ª** - ×›×œ ××¤×©×¨×•×ª ×—×™×™×‘×ª ×œ×”×™×•×ª ×”×’×™×•× ×™×ª ×•×××™× ×”
4. **×ª×©×•×‘×” × ×›×•× ×” ××—×ª** - ×¨×§ ×ª×©×•×‘×” ××—×ª × ×›×•× ×”, ×”×©××¨ ××•×˜×¢×•×ª ×‘×‘×™×¨×•×¨
5. **×”×¡×‘×¨ ××¤×•×¨×˜** - ×›×œ ×©××œ×” ××œ×•×•×” ×‘×”×¡×‘×¨ ××§×™×£ ×”××‘××¨ ×œ××” ×”×ª×©×•×‘×” × ×›×•× ×”
6. **×”×¤× ×™×” ×—×•×§×™×ª** - ×¦×™×•×Ÿ ×”××§×•×¨ ×”××©×¤×˜×™ ×”××“×•×™×§ (×—×•×§, ×ª×§× ×”, ×¡×¢×™×£)

**×˜×¨××™× ×•×œ×•×’×™×”:**
×”×©×ª××© ×‘×˜×¨××™× ×•×œ×•×’×™×” ××©×¤×˜×™×ª ××§×¦×•×¢×™×ª ×‘×¢×‘×¨×™×ª, ×›×¤×™ ×©××•×¤×™×¢×” ×‘×—×•×§ × ×™×™×¨×•×ª ×¢×¨×š ×•×‘×ª×§× ×•×ª ×”×¨×œ×•×•× ×˜×™×•×ª.

**××™×›×•×ª ×”×¢×‘×¨×™×ª - ×—×©×•×‘ ×××•×“:**
- ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ×ª×§× ×™×ª, ×‘×¨×•×¨×” ×•××§×¦×•×¢×™×ª
- ×”×§×¤×“ ×¢×œ ×“×§×“×•×§, ×›×ª×™×‘ ×•× ×™×¡×•×— × ×›×•×Ÿ
- ×”×©×ª××© ×‘××©×¤×˜×™× ×–×•×¨××™× ×•×§×œ×™× ×œ×”×‘× ×”
- ×”×™×× ×¢ ××©×’×™××•×ª ×“×§×“×•×§ ××• ×¡×’× ×•×Ÿ
- ×›×ª×•×‘ ×›××• ×¢×•×¨×š ×˜×§×¡×˜ ××§×¦×•×¢×™

**×¨××•×ª ×§×•×©×™:**
- easy: ×¡×™×¤×•×¨×™× ×¤×©×•×˜×™× ×¢× ×™×™×©×•× ×™×©×™×¨ ×©×œ ×—×•×§ ×‘×¡×™×¡×™
- medium: ×¡×™×¤×•×¨×™× ××•×¨×›×‘×™× ×™×•×ª×¨ ×¢× ×™×™×©×•× ×—×•×§×™× ×•×”×‘× ×ª ××¦×‘×™×
- hard: ××§×¨×™× ××•×¨×›×‘×™× ×¢× ×©×™×œ×•×‘ ××¡×¤×¨ × ×•×©××™× ×•×“×™×œ××•×ª ××ª×™×•×ª"""

    def generate_quiz(
        self,
        question_count: int = 10,
        topic: Optional[str] = None,
        difficulty: Optional[str] = None,
        focus_areas: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Generate a personalized quiz

        Args:
            question_count: Number of questions to generate (default: 10)
            topic: Specific topic (e.g., "××™×“×¢ ×¤× ×™×", "×—×•×‘×•×ª ×’×™×œ×•×™") or None for general
            difficulty: Difficulty level ("easy", "medium", "hard") or None for mixed
            focus_areas: List of weak point topics to focus on

        Returns:
            Dict with:
                - questions: List of generated questions
                - metadata: Quiz metadata
                - reference_sources: Legal sources used
        """
        self.log(f"Generating quiz: {question_count} questions, topic={topic}, difficulty={difficulty}")

        # Step 1: Get reference questions from Exam RAG
        reference_questions = self._get_reference_questions(
            topic=topic,
            difficulty=difficulty,
            focus_areas=focus_areas,
            k=15  # Get 15 reference questions for inspiration
        )

        self.log(f"Retrieved {len(reference_questions)} reference questions")

        # Step 2: Get legal context from Legal RAG
        legal_context = self._get_legal_context(
            topic=topic,
            focus_areas=focus_areas,
            k=20  # Maximum legal chunks for accuracy
        )

        self.log("Retrieved legal context")

        # Step 3: Generate questions using thinking model
        # Generate 2x requested to account for validation rejections
        generation_count = question_count * 2
        self.log(f"Generating {generation_count} questions (2x requested for validation)")

        generated_questions = self._generate_questions_with_llm(
            question_count=generation_count,
            topic=topic,
            difficulty=difficulty,
            reference_questions=reference_questions,
            legal_context=legal_context
        )

        self.log(f"Generated {len(generated_questions)} questions")

        # Step 4: Structural validation
        structurally_valid = self._validate_structure(generated_questions)
        self.log(f"Structurally valid: {len(structurally_valid)} questions")

        # Step 5: Legal Expert validation (CRITICAL for accuracy)
        expert_validated = self._validate_with_legal_expert(structurally_valid)
        self.log(f"Legal Expert validated: {len(expert_validated)} questions")

        # Step 6: Final enrichment
        validated_questions = self._final_enrichment(expert_validated)
        self.log(f"Expert validated: {len(validated_questions)} questions")

        # Limit to requested count (take best ones)
        final_questions = validated_questions[:question_count]

        # Warn if we didn't get enough
        if len(final_questions) < question_count:
            self.log(f"âš ï¸  Only {len(final_questions)}/{question_count} questions passed validation")
            self.log(f"   This is normal - validation is strict for accuracy")

        self.log(f"Final output: {len(final_questions)} questions")

        return {
            'questions': final_questions,
            'metadata': {
                'question_count': len(final_questions),
                'requested_count': question_count,
                'topic': topic or '×›×œ×œ×™',
                'difficulty': difficulty or '××¢×•×¨×‘',
                'focus_areas': focus_areas or [],
                'generated_at': self._get_timestamp(),
                'validation_stats': {
                    'generated': len(generated_questions),
                    'structurally_valid': len(structurally_valid),
                    'expert_validated': len(expert_validated),
                    'final_count': len(final_questions)
                }
            },
            'reference_sources': self._extract_legal_sources(legal_context)
        }

    def _get_reference_questions(
        self,
        topic: Optional[str],
        difficulty: Optional[str],
        focus_areas: Optional[List[str]],
        k: int = 15
    ) -> List[Dict]:
        """
        Get reference questions from Exam RAG

        Args:
            topic: Topic to search for
            difficulty: Difficulty level
            focus_areas: Specific focus areas
            k: Number of questions to retrieve

        Returns:
            List of reference questions
        """
        # Build search query
        if focus_areas:
            # Search for specific weak points
            queries = [f"×©××œ×•×ª ×‘× ×•×©× {area}" for area in focus_areas]
            all_results = []
            for query in queries:
                results = self.exam_rag.search(query, k=k//len(focus_areas) + 1)
                all_results.extend(results)
            return all_results[:k]

        elif topic:
            # Search for specific topic
            query = f"×©××œ×•×ª ×‘× ×•×©× {topic}"
            return self.exam_rag.search(query, k=k)

        else:
            # General search - get diverse questions
            query = "×©××œ×•×ª ××‘×—×Ÿ × ×™×™×¨×•×ª ×¢×¨×š ×•××ª×™×§×”"
            return self.exam_rag.search(query, k=k)

    def _get_legal_context(
        self,
        topic: Optional[str],
        focus_areas: Optional[List[str]],
        k: int = 20
    ) -> str:
        """
        Get legal context from Legal RAG

        Args:
            topic: Topic to search for
            focus_areas: Specific focus areas
            k: Number of chunks to retrieve

        Returns:
            Combined legal context string
        """
        # Build comprehensive query
        if focus_areas:
            queries = focus_areas
        elif topic:
            queries = [topic]
        else:
            queries = ["×—×•×§ × ×™×™×¨×•×ª ×¢×¨×š", "××™×¡×•×¨ ×× ×™×¤×•×œ×¦×™×”", "××™×“×¢ ×¤× ×™×", "×—×•×‘×•×ª ×’×™×œ×•×™"]

        all_context = []
        chunks_per_query = max(k // len(queries), 5)

        for query in queries:
            context = self.retrieve_context(query, k=chunks_per_query)
            all_context.append(context)

        return "\n\n---\n\n".join(all_context)

    def _generate_questions_with_llm(
        self,
        question_count: int,
        topic: Optional[str],
        difficulty: Optional[str],
        reference_questions: List[Dict],
        legal_context: str
    ) -> List[Dict]:
        """
        Generate questions using LLM with full context

        Args:
            question_count: Number of questions to generate
            topic: Topic focus
            difficulty: Difficulty level
            reference_questions: Reference questions from exam database
            legal_context: Legal context from legal database

        Returns:
            List of generated questions
        """
        # Format reference questions
        reference_text = self._format_reference_questions(reference_questions[:5])

        # Build generation prompt
        generation_prompt = f"""××ª×” ×™×•×¦×¨ ×©××œ×•×ª ××‘×—×Ÿ ×‘× ×•×©× × ×™×™×¨×•×ª ×¢×¨×š ×•××ª×™×§×” ××§×¦×•×¢×™×ª.

**×“×¨×™×©×•×ª:**
- ×¦×•×¨ **{question_count}** ×©××œ×•×ª ××‘×—×Ÿ ××™×›×•×ª×™×•×ª
- × ×•×©×: {topic if topic else '×›×œ×œ×™ (×›×™×¡×•×™ ×›×œ ×”× ×•×©××™×)'}
- ×¨××ª ×§×•×©×™: {difficulty if difficulty else '××¢×•×¨×‘ (easy, medium, hard)'}

**×—×©×•×‘ ×××•×“ - ×¡×’× ×•×Ÿ ×©××œ×•×ª:**
×™×© ×œ×™×™×¦×¨ **×©× ×™ ×¡×•×’×™ ×©××œ×•×ª**:

**70-80% ×©××œ×•×ª ×¡×™×¤×•×¨** (×¢× ×“××•×™×•×ª ×•×ª×¨×—×™×©×™×):
âœ… "×¨×•× ×™, ×™×•×¢×¥ ×”×©×§×¢×•×ª ×‘×—×‘×¨×ª ×¤×™× × ×¡-×˜×§ ×‘×¢\"×, ×§×™×‘×œ ×©×™×—×” ××œ×§×•×— ×•×ª×™×§ ×©×œ×•. ×”×œ×§×•×— ×‘×™×§×©..."
âœ… "×©×•×§×™ ×•××•×§×™ ×× ×”×œ×™× ×§×¨×Ÿ ×”×©×§×¢×•×ª ××©×•×ª×¤×ª ×‘×©× '××•×¤×§-×”×©×§×¢×•×ª'. ×‘×™×©×™×‘×ª ×”× ×”×œ×” ×”××—×¨×•× ×” ×”×ª×¢×•×¨×¨ ×“×™×•×Ÿ ×¢×œ..."
âœ… "×“×™×œ×Ÿ ×¢×•×‘×“×ª ×›×× ×”×œ×ª ×ª×™×§×™× ×‘×‘× ×§ ×™×©×¨××œ-×”×©×§×¢×•×ª. ××—×“ ×”×œ×§×•×—×•×ª ×©×œ×”, ×‘×¢×œ ×—×‘×¨×ª ×˜×›× ×•-×¡×•×œ×•×©×Ÿ ×‘×¢\"×..."

**20-30% ×©××œ×•×ª ×‘×¡×™×¡×™×•×ª** (×”×‘× ×” ×‘×¡×™×¡×™×ª):
âœ… "××”×™ ×”×’×“×¨×ª '××™×“×¢ ×¤× ×™×' ×œ×¤×™ ×—×•×§ × ×™×™×¨×•×ª ×¢×¨×š?"
âœ… "×‘××™×–×” ×¡×¢×™×£ ×‘×—×•×§ ××•×’×“×¨ ××™×¡×•×¨ ×× ×™×¤×•×œ×¦×™×”?"
âœ… "××”×Ÿ ×—×•×‘×•×ª ×”×’×™×œ×•×™ ×©×œ ×—×‘×¨×” ×¦×™×‘×•×¨×™×ª?"

**×—×©×•×‘ ×××•×“ - ×©××•×ª ×—×‘×¨×•×ª:**
âŒ **××¡×•×¨** ×œ×”×©×ª××© ×‘×©××•×ª ×—×‘×¨×•×ª ×××™×ª×™×•×ª: ×’×•×’×œ, ××¤×œ, ××™×§×¨×•×¡×•×¤×˜, ×××–×•×Ÿ, ×¤×™×™×¡×‘×•×§, ×˜×¡×œ×”, × ×˜×¤×œ×™×§×¡
âœ… **×—×•×‘×”** ×œ×”×©×ª××© ×‘×©××•×ª ×—×‘×¨×•×ª **×‘×“×•×™×™×**: "×˜×›× ×•-×¡×•×œ×•×©×Ÿ ×‘×¢\"×", "×¤×™× × ×¡-×˜×§ ×”×©×§×¢×•×ª", "×“×™×’×™×˜×œ-×•×™×–'×Ÿ", "××•×¤×˜×™-×¡×™×™×‘×¨ ×‘×¢\"×"
**×¡×™×‘×”**: ×”×’× ×” ××¤× ×™ ×‘×¢×™×•×ª ×–×›×•×™×•×ª ×™×•×¦×¨×™× (copyright)

**×”×§×©×¨ ××©×¤×˜×™ ×¨×œ×•×•× ×˜×™:**
{legal_context[:8000]}

**×“×•×’×××•×ª ×œ×©××œ×•×ª ×§×™×™××•×ª (×œ××“ ××”×¡×’× ×•×Ÿ ×©×œ×”×Ÿ!):**
{reference_text}

**×¤×•×¨××˜ ×¤×œ×˜ - JSON Array:**
```json
[
  {{
    "question_number": 1,
    "question_text": "×˜×§×¡×˜ ×”×©××œ×” ×”××œ×",
    "options": {{
      "A": "××¤×©×¨×•×ª × - ×˜×§×¡×˜ ××œ×",
      "B": "××¤×©×¨×•×ª ×‘ - ×˜×§×¡×˜ ××œ×",
      "C": "××¤×©×¨×•×ª ×’ - ×˜×§×¡×˜ ××œ×",
      "D": "××¤×©×¨×•×ª ×“ - ×˜×§×¡×˜ ××œ×",
      "E": "××¤×©×¨×•×ª ×” - ×˜×§×¡×˜ ××œ×"
    }},
    "correct_answer": "B",
    "explanation": "×”×¡×‘×¨ ××¤×•×¨×˜ ××“×•×¢ ×”×ª×©×•×‘×” ×”× ×›×•× ×” ×”×™× B, ×›×•×œ×œ ×”×¤× ×™×” ×œ××§×•×¨×•×ª ××©×¤×˜×™×™×. ×”×¡×‘×¨ ×’× ×œ××” ×”×©××¨ ×©×’×•×™×•×ª.",
    "topic": "×”× ×•×©× ×”×¢×™×§×¨×™ ×©×œ ×”×©××œ×”",
    "difficulty": "easy/medium/hard",
    "legal_reference": "×—×•×§ × ×™×™×¨×•×ª ×¢×¨×š, ×ª×©×›\\"×—-1968, ×¡×¢×™×£ X"
  }},
  ...
]
```

**×—×©×•×‘ ×××•×“:**
1. **×¢×‘×¨×™×ª ××§×¦×•×¢×™×ª!** - ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ×ª×§× ×™×ª, ×‘×¨×•×¨×” ×•××§×¦×•×¢×™×ª. ×”×§×¤×“ ×¢×œ ×“×§×“×•×§, ×›×ª×™×‘ ×•× ×™×¡×•×— × ×›×•×Ÿ!
2. **×©××œ×•×ª ××¢×•×¨×‘×•×ª!** - 70-80% ×©××œ×•×ª ×¡×™×¤×•×¨ ×¢× ×ª×¨×—×™×©×™× + 20-30% ×©××œ×•×ª ×‘×¡×™×¡×™×•×ª
3. **×©××•×ª ×—×‘×¨×•×ª ×‘×“×•×™×™× ×‘×œ×‘×“!** - ××¡×•×¨ ×œ×”×©×ª××© ×‘×’×•×’×œ, ××¤×œ, ××™×§×¨×•×¡×•×¤×˜, ×××–×•×Ÿ, ×¤×™×™×¡×‘×•×§ ×•×›×•'. ×¨×§ ×©××•×ª ×‘×“×•×™×™×!
4. **×‘×“×™×•×§ 5 ××¤×©×¨×•×™×•×ª (A, B, C, D, E)** - ××¡×•×¨ ×œ×›×ª×•×‘ 4 ××• 6 ××¤×©×¨×•×™×•×ª! ×—×™×™×‘ ×œ×”×™×•×ª ×‘×“×™×•×§ 5!
5. ×›×œ ×©××œ×” ×—×™×™×‘×ª ×œ×”×™×•×ª ××“×•×™×§×ª ××©×¤×˜×™×ª ×•××‘×•×¡×¡×ª ×¢×œ ×”×—×•×§ ×”×™×©×¨××œ×™
6. ×¨×§ ×ª×©×•×‘×” ××—×ª × ×›×•× ×” - ×”×©××¨ ×—×™×™×‘×•×ª ×œ×”×™×•×ª ×©×’×•×™×•×ª ×‘×‘×™×¨×•×¨
7. ×”×¡×‘×¨ ×—×™×™×‘ ×œ×”×™×•×ª ××¤×•×¨×˜ ×•×œ×”×¡×‘×™×¨ ×’× ×œ××” ×”×©××¨ ×©×’×•×™×•×ª
8. ×”×¤× ×™×” ×—×•×§×™×ª ×—×™×™×‘×ª ×œ×”×™×•×ª ××“×•×™×§×ª (×—×•×§, ×¡×¢×™×£)
9. × ×•×©× ×—×™×™×‘ ×œ×”×™×•×ª ×ª××¦×™×ª×™ ×•××“×•×™×§
10. ××œ ×ª×¢×ª×™×§ ×©××œ×•×ª ×§×™×™××•×ª - ×¦×•×¨ ×©××œ×•×ª ×—×“×©×•×ª ×‘×”×©×¨××ª×Ÿ
11. ×× ×™×© ×œ×š ×¨×§ 4 ×¨×¢×™×•× ×•×ª ×œ××¤×©×¨×•×™×•×ª, ×”×•×¡×£ ××¤×©×¨×•×ª ×—××™×©×™×ª (×œ××©×œ: "×›×œ ×”×ª×©×•×‘×•×ª × ×›×•× ×•×ª", "××£ ××—×“ ××”× \\"×œ", ×•×›×“')
12. **×”×©×ª××© ×‘×©××•×ª ××’×•×•× ×™×**: ×©×•×§×™, ××•×§×™, ×“×™×œ×Ÿ, ×‘×¨× ×“×”, ×¨×•× ×™, ×’×™×, ××™×›×œ, ×“× ×”, ×™×•×¡×™, ×ª××¨, ××‘×™, × ×•×¢×”

×¦×•×¨ ×¢×›×©×™×• {question_count} ×©××œ×•×ª ××™×›×•×ª×™×•×ª ×‘×¤×•×¨××˜ JSON - ×–×›×•×¨: 70-80% ×¡×™×¤×•×¨×™×, 20-30% ×©××œ×•×ª ×‘×¡×™×¡×™×•×ª!"""

        try:
            # Call thinking model with maximum context
            response = self.client.chat.completions.create(
                model=THINKING_MODEL,
                messages=[
                    {"role": "system", "content": self._build_system_prompt()},
                    {"role": "user", "content": generation_prompt}
                ],
                temperature=0.7  # Creative but controlled
            )

            result_text = response.choices[0].message.content.strip()

            # Parse JSON response
            questions = self._parse_json_from_text(result_text)

            if not isinstance(questions, list):
                self.log("âš ï¸  LLM did not return a list, attempting to extract")
                questions = []

            return questions

        except Exception as e:
            self.log(f"âŒ Error generating questions: {e}")
            return []

    def _format_reference_questions(self, questions: List[Dict]) -> str:
        """Format reference questions for prompt"""
        formatted = []
        for i, q in enumerate(questions[:5], 1):
            formatted.append(f"""
×“×•×’××” {i}:
×©××œ×”: {q.get('question_text', '')[:200]}...
× ×•×©×: {q.get('topic', '×œ× ×™×“×•×¢')}
×¨××ª ×§×•×©×™: {q.get('difficulty', 'medium')}
""")
        return "\n".join(formatted)

    def _validate_structure(self, questions: List[Dict]) -> List[Dict]:
        """
        Stage 1: Strict structural validation

        CRITICAL: All questions MUST have exactly 5 options (A-E)

        Args:
            questions: Generated questions

        Returns:
            Structurally valid questions only
        """
        validated = []

        for i, q in enumerate(questions, 1):
            # Check required fields
            required = ['question_text', 'options', 'correct_answer', 'explanation', 'topic']
            missing = [f for f in required if f not in q or not q[f]]

            if missing:
                self.log(f"âš ï¸  Q{i}: Missing fields: {missing}")
                continue

            # CRITICAL: Must have exactly 5 options (A, B, C, D, E)
            options = q.get('options', {})
            if not isinstance(options, dict):
                self.log(f"âš ï¸  Q{i}: Options is not a dict")
                continue

            required_options = ['A', 'B', 'C', 'D', 'E']
            missing_opts = [opt for opt in required_options if opt not in options or not str(options[opt]).strip()]
            extra_opts = [opt for opt in options.keys() if opt not in required_options]

            if missing_opts:
                self.log(f"âŒ Q{i}: MISSING OPTIONS: {missing_opts} - REJECTED")
                continue

            if extra_opts:
                self.log(f"âš ï¸  Q{i}: Extra options: {extra_opts} - removing")
                # Keep only A-E
                q['options'] = {k: v for k, v in options.items() if k in required_options}

            if len(q['options']) != 5:
                self.log(f"âŒ Q{i}: Has {len(q['options'])} options, need 5 - REJECTED")
                continue

            # Validate correct_answer is A-E
            correct = q.get('correct_answer', '').upper()
            if correct not in required_options:
                self.log(f"âš ï¸  Q{i}: Invalid correct answer '{correct}'")
                continue

            q['correct_answer'] = correct

            # Normalize difficulty
            if q.get('difficulty') not in ['easy', 'medium', 'hard']:
                q['difficulty'] = 'medium'

            validated.append(q)

        return validated

    def _validate_with_legal_expert(self, questions: List[Dict]) -> List[Dict]:
        """
        Stage 2: Legal Expert validation

        Tests each question with Legal Expert Agent to ensure:
        1. Legal Expert can answer correctly (question is clear)
        2. Answer matches what we think is correct
        3. Question is legally accurate
        4. Question is relevant to securities law

        This is CRITICAL for exam accuracy.

        Args:
            questions: Structurally valid questions

        Returns:
            Expert-validated questions only
        """
        validated = []

        for i, q in enumerate(questions, 1):
            self.log(f"ğŸ§ª Testing Q{i} with Legal Expert...")

            # Build validation query (don't tell correct answer)
            validation_query = f"""××ª×” ××§×‘×œ ×©××œ×ª ××‘×—×Ÿ ×‘× ×•×©× × ×™×™×¨×•×ª ×¢×¨×š ×•××ª×™×§×” ××§×¦×•×¢×™×ª.
×× × ×¢× ×” ×¢×œ ×”×©××œ×” ×•×”×¡×‘×¨ ××ª ×ª×©×•×‘×ª×š.

**×©××œ×”:**
{q['question_text']}

**××¤×©×¨×•×™×•×ª:**
A. {q['options']['A']}
B. {q['options']['B']}
C. {q['options']['C']}
D. {q['options']['D']}
E. {q['options']['E']}

**×“×¨×™×©×•×ª:**
1. ×‘×—×¨ ×ª×©×•×‘×” ××—×ª × ×›×•× ×” (A/B/C/D/E ×‘×œ×‘×“)
2. ×”×¡×‘×¨ ×œ××” ×–×• ×”×ª×©×•×‘×” ×”× ×›×•× ×”
3. ×¦×™×™×Ÿ ×¨××ª ×‘×™×˜×—×•×Ÿ (high/medium/low)

×”×©×‘ ×‘×¤×•×¨××˜ JSON:
```json
{{
  "answer": "A",
  "explanation": "×”×¡×‘×¨ ××¤×•×¨×˜...",
  "confidence": "high",
  "legal_reference": "×—×•×§ X, ×¡×¢×™×£ Y"
}}
```"""

            try:
                # Query Legal Expert using process_with_rag
                response = self.legal_expert.process_with_rag(
                    query=validation_query,
                    k=15  # Use top 15 legal chunks for accuracy
                )

                result = response.get('answer', '')

                # Debug: Show what Legal Expert returned
                if i == 1:  # Only log first question to avoid spam
                    self.log(f"ğŸ“ Legal Expert raw response (Q1): {result[:200]}...")

                # Parse response
                validation_data = self._parse_json_from_text(result)

                if not validation_data or not isinstance(validation_data, dict):
                    self.log(f"âš ï¸  Q{i}: Failed to parse Legal Expert response")
                    if i == 1:  # Show why it failed for first question
                        self.log(f"   Parsed data: {validation_data}")
                    continue

                agent_answer = validation_data.get('answer', '').upper()
                correct_answer = q['correct_answer'].upper()
                confidence = validation_data.get('confidence', 'low')

                # Check if Legal Expert got it right
                if agent_answer != correct_answer:
                    self.log(f"âŒ Q{i}: Legal Expert answered {agent_answer}, correct is {correct_answer} - REJECTED")
                    self.log(f"   Reason: Question may be ambiguous or incorrect")
                    continue

                # Check confidence
                if confidence == 'low':
                    self.log(f"âš ï¸  Q{i}: Legal Expert has low confidence - REJECTED")
                    continue

                # Question passed validation!
                self.log(f"âœ… Q{i}: Legal Expert validated (confidence: {confidence})")

                # Enhance explanation with Legal Expert's reasoning
                q['expert_validation'] = {
                    'validated': True,
                    'expert_explanation': validation_data.get('explanation', ''),
                    'expert_reference': validation_data.get('legal_reference', ''),
                    'confidence': confidence
                }

                validated.append(q)

            except Exception as e:
                self.log(f"âŒ Q{i}: Validation error: {e}")
                continue

        return validated

    def _final_enrichment(self, questions: List[Dict]) -> List[Dict]:
        """
        Stage 3: Final enrichment and metadata

        Args:
            questions: Expert-validated questions

        Returns:
            Final enriched questions
        """
        enriched = []

        for i, q in enumerate(questions, 1):
            # Add final metadata
            q['generated'] = True
            q['question_number'] = i
            q['validated_by_expert'] = True

            enriched.append(q)

        return enriched


    def _parse_json_from_text(self, text: str):
        """Extract JSON from LLM response (handles both arrays and objects)"""
        # Try to find JSON in code blocks (array)
        json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # Try to find JSON in code blocks (object)
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # Try to find JSON array without code blocks
        json_match = re.search(r'(\[.*?\])', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # Try to find JSON object without code blocks
        json_match = re.search(r'(\{.*?\})', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # Try parsing entire text as JSON
        try:
            return json.loads(text)
        except:
            pass

        return None

    def _extract_legal_sources(self, context: str) -> List[str]:
        """Extract legal source references from context"""
        sources = []

        # Look for source markers in context
        parts = context.split("---")
        for part in parts[:5]:  # First 5 sources
            if "[××§×•×¨" in part:
                lines = part.strip().split("\n")
                if lines:
                    header = lines[0]
                    try:
                        doc_part = header.split("]")[1].strip()
                        sources.append(doc_part)
                    except:
                        pass

        return sources

    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process quiz generation request

        Args:
            input_data: Dict with:
                - question_count: int (default: 10)
                - topic: str (optional)
                - difficulty: str (optional)
                - focus_areas: List[str] (optional)

        Returns:
            Generated quiz
        """
        question_count = input_data.get('question_count', 10)
        topic = input_data.get('topic')
        difficulty = input_data.get('difficulty')
        focus_areas = input_data.get('focus_areas')

        return self.generate_quiz(
            question_count=question_count,
            topic=topic,
            difficulty=difficulty,
            focus_areas=focus_areas
        )
